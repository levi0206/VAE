{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "from typing import List\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    # Input dimension of encoder\n",
    "    # We'll flat the tensor\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_aug_sig, epoch, batch_size, hidden_dims: List, device) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.x_aug_sig = x_aug_sig\n",
    "        print(\"Input tensor shape: {}\".format(x_aug_sig.shape))\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Assume len(hidden_dims)=3.\n",
    "        self.encoder_mu = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.encoder_sigma = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[2],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[0]),\n",
    "        )\n",
    "\n",
    "        # To device\n",
    "        self.encoder_mu.to(device)\n",
    "        self.encoder_sigma.to(device)\n",
    "        self.decoder.to(device)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x_flatten = x.view(self.batch_size,-1)\n",
    "        mean = self.encoder_mu(x_flatten)\n",
    "        log_var = self.encoder_sigma(x_flatten)\n",
    "        # Clipping\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        noise = torch.randn(self.batch_size,mean.shape[1]).to(self.device)\n",
    "        z = mean + torch.exp(0.5*log_var).mul(noise)\n",
    "        return mean, log_var, z\n",
    "        \n",
    "    def decode(self,z):\n",
    "        reconstructed_data = self.decoder(z)\n",
    "        return reconstructed_data\n",
    "\n",
    "    def loss(self,mean,log_var,sample_data,reconstructed_data):\n",
    "        # Reconstruction loss \n",
    "        recon_loss = F.mse_loss(sample_data, reconstructed_data, reduction='mean')\n",
    "        # print(recon_loss.item())\n",
    "        # KL divergence\n",
    "        kl_loss = 0.5 * ((mean.pow(2) + log_var.exp() - 1 - log_var).mean()).sum()\n",
    "        # Total VAE loss\n",
    "        loss = recon_loss + kl_loss\n",
    "        return loss\n",
    "    \n",
    "def train(model,optimizer):\n",
    "    early_stop = 500\n",
    "    cnt = 0\n",
    "    min_loss = float('inf')\n",
    "    for i in range(model.epoch):\n",
    "        # Sample time indices of size equal to the batch size\n",
    "        # From sefl.x_aug_sig\n",
    "        time_indics = sample_indices(model.x_aug_sig.shape[0],model.batch_size,\"cuda\")\n",
    "        sample_data = model.x_aug_sig[time_indics]\n",
    "        # Encode \n",
    "        mean, log_var, z = model.encode(sample_data)\n",
    "        # Decode\n",
    "        reconstructed_data = model.decode(z)\n",
    "        # Calculate loss\n",
    "        loss = model.loss(mean,log_var,sample_data.view(model.batch_size,-1),reconstructed_data)\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch {} loss {}\".format(i,loss.item()))\n",
    "        if loss.item()<min_loss:\n",
    "            min_loss = loss.item()\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt>early_stop:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([985, 39, 4])\n",
      "VAE(\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=20, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=20, out_features=5, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (encoder_sigma): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=20, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=20, out_features=5, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=20, out_features=156, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "epoch = 10001\n",
    "hidden_dims = [input_dim,20,5]\n",
    "VAE = VAE(x_aug_sig=x_aug_sig,epoch=epoch,batch_size=batch_size,hidden_dims=hidden_dims,device='cuda')\n",
    "print(VAE)\n",
    "optimizer = torch.optim.Adam(VAE.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After add a line ``log_var = torch.clamp(log_var, min=-10, max=10)``, the loss is no longer ``nan``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 9222623.0\n",
      "Epoch 500 loss 6971883.0\n",
      "Epoch 1000 loss 2931123.0\n",
      "Epoch 1500 loss 1180820.625\n",
      "Epoch 2000 loss 663845.0625\n",
      "Epoch 2500 loss 543780.9375\n",
      "Epoch 3000 loss 417477.625\n",
      "Epoch 3500 loss 355811.8125\n",
      "Epoch 4000 loss 290372.5\n",
      "Epoch 4500 loss 259753.8125\n",
      "Epoch 5000 loss 193912.515625\n",
      "Epoch 5500 loss 184750.0\n",
      "Epoch 6000 loss 160211.75\n",
      "Epoch 6500 loss 150014.203125\n",
      "Epoch 7000 loss 128494.3671875\n",
      "Epoch 7500 loss 105953.46875\n",
      "Epoch 8000 loss 101809.640625\n",
      "Epoch 8500 loss 86966.171875\n",
      "Epoch 9000 loss 59273.8515625\n",
      "Epoch 9500 loss 49072.26171875\n",
      "Epoch 10000 loss 43448.28515625\n"
     ]
    }
   ],
   "source": [
    "train(VAE,optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
