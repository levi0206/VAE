{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "from typing import List\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    # Input dimension of encoder\n",
    "    # We'll flat the tensor\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_aug_sig, epoch, batch_size, hidden_dims: List, device) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.x_aug_sig = x_aug_sig\n",
    "        print(\"Input tensor shape: {}\".format(x_aug_sig.shape))\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Assume len(hidden_dims)=3.\n",
    "        self.encoder_mu = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.encoder_sigma = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[2],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[0]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        # To device\n",
    "        self.encoder_mu.to(device)\n",
    "        self.encoder_sigma.to(device)\n",
    "        self.decoder.to(device)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x_flatten = x.view(x.shape[0],-1)\n",
    "        mean = self.encoder_mu(x_flatten)\n",
    "        log_var = self.encoder_sigma(x_flatten)\n",
    "        # Clipping\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        noise = torch.randn(x.shape[0],mean.shape[1]).to(self.device)\n",
    "        z = mean + torch.exp(0.5*log_var).mul(noise)\n",
    "        return mean, log_var, z\n",
    "        \n",
    "    def decode(self,z):\n",
    "        reconstructed_data = self.decoder(z)\n",
    "        return reconstructed_data\n",
    "\n",
    "    def loss(self,mean,log_var,sample_data,reconstructed_data):\n",
    "        # Reconstruction loss \n",
    "        recon_loss = F.mse_loss(sample_data, reconstructed_data, reduction='mean')\n",
    "        # print(recon_loss.item())\n",
    "        # KL divergence\n",
    "        kl_loss = 0.5 * ((mean.pow(2) + log_var.exp() - 1 - log_var).mean()).sum()\n",
    "        # Total VAE loss\n",
    "        loss = recon_loss + kl_loss\n",
    "        return loss\n",
    "    \n",
    "    def generate(self,x: torch.Tensor):\n",
    "        _, _, z = self.encode(x)\n",
    "        reconstructed_data = self.decode(z)\n",
    "        return reconstructed_data\n",
    "    \n",
    "def train(model,optimizer):\n",
    "    early_stop = 500\n",
    "    cnt = 0\n",
    "    min_loss = float('inf')\n",
    "    for i in range(model.epoch):\n",
    "        # Sample time indices of size equal to the batch size\n",
    "        # From sefl.x_aug_sig\n",
    "        time_indics = sample_indices(model.x_aug_sig.shape[0],model.batch_size,\"cuda\")\n",
    "        sample_data = model.x_aug_sig[time_indics]\n",
    "        # Encode \n",
    "        mean, log_var, z = model.encode(sample_data)\n",
    "        # Decode\n",
    "        reconstructed_data = model.decode(z)\n",
    "        # Calculate loss\n",
    "        loss = model.loss(mean,log_var,sample_data.view(model.batch_size,-1),reconstructed_data)\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch {} loss {}\".format(i,loss.item()))\n",
    "        # Early stop\n",
    "        if loss.item()<min_loss:\n",
    "            min_loss = loss.item()\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt>early_stop:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([985, 39, 4])\n",
      "VAE(\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (encoder_sigma): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=156, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 200\n",
    "epoch = 20001\n",
    "hidden_dims = [input_dim,12,3]\n",
    "VAE = VAE(x_aug_sig=x_aug_sig,epoch=epoch,batch_size=batch_size,hidden_dims=hidden_dims,device='cuda')\n",
    "print(VAE)\n",
    "optimizer = torch.optim.Adam(VAE.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After add a line ``log_var = torch.clamp(log_var, min=-10, max=10)``, the loss is no longer ``nan``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 9894051.0\n",
      "Epoch 500 loss 8365525.0\n",
      "Epoch 1000 loss 6203100.0\n",
      "Epoch 1500 loss 3961619.25\n",
      "Epoch 2000 loss 2640961.0\n",
      "Epoch 2500 loss 1747889.75\n",
      "Epoch 3000 loss 1363460.875\n",
      "Epoch 3500 loss 1040551.75\n",
      "Epoch 4000 loss 826465.875\n",
      "Epoch 4500 loss 705351.375\n",
      "Epoch 5000 loss 600597.5625\n",
      "Epoch 5500 loss 487357.5625\n",
      "Epoch 6000 loss 410720.875\n",
      "Epoch 6500 loss 361692.125\n",
      "Epoch 7000 loss 309668.25\n",
      "Epoch 7500 loss 266744.25\n",
      "Epoch 8000 loss 218051.59375\n",
      "Epoch 8500 loss 197517.84375\n",
      "Epoch 9000 loss 174988.234375\n",
      "Epoch 9500 loss 148027.59375\n",
      "Epoch 10000 loss 131575.25\n",
      "Epoch 10500 loss 114032.9609375\n",
      "Epoch 11000 loss 103479.75\n",
      "Epoch 11500 loss 91329.5859375\n",
      "Epoch 12000 loss 84690.8203125\n",
      "Epoch 12500 loss 73483.078125\n",
      "Epoch 13000 loss 67662.390625\n",
      "Epoch 13500 loss 59745.98046875\n",
      "Epoch 14000 loss 56640.38671875\n",
      "Epoch 14500 loss 51677.4140625\n",
      "Epoch 15000 loss 47065.0078125\n",
      "Epoch 15500 loss 42823.01171875\n",
      "Epoch 16000 loss 38336.97265625\n",
      "Epoch 16500 loss 38253.203125\n",
      "Epoch 17000 loss 36277.67578125\n",
      "Epoch 17500 loss 34047.00390625\n",
      "Epoch 18000 loss 33957.3125\n",
      "Epoch 18500 loss 31696.3203125\n",
      "Epoch 19000 loss 30362.52734375\n",
      "Epoch 19500 loss 28963.275390625\n",
      "Epoch 20000 loss 26444.443359375\n"
     ]
    }
   ],
   "source": [
    "train(VAE,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample some data and calculate its MMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 156, 1])\n"
     ]
    }
   ],
   "source": [
    "time_indices = sample_indices(x_aug_sig.shape[0],128,'cpu')\n",
    "sampled_data = x_aug_sig[time_indices]\n",
    "generated_data = VAE.generate(sampled_data).to('cpu')\n",
    "print(generated_data.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from lib.mmd import mmd_loss,SignatureKernel\n",
    "kernel = SignatureKernel(4,None)\n",
    "mmd = mmd_loss(sampled_data.unsqueeze(2),generated_data.unsqueeze(2),kernel)\n",
    "print(mmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
