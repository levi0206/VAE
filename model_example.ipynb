{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "from typing import List\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_aug_sig, epoch, batch_size, hidden_dims: List, device) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.x_aug_sig = x_aug_sig\n",
    "        print(\"Inpust tensor shape: {}\".format(x_aug_sig.shape))\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dims[i],hidden_dims[i+1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "        self.encoder_mu = nn.Sequential(*modules)\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dims[i],hidden_dims[i+1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "        self.encoder_sigma = nn.Sequential(*modules)\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims)-1,0,-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dims[i],hidden_dims[i-1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        # To device\n",
    "        self.encoder_mu.to(device)\n",
    "        self.encoder_sigma.to(device)\n",
    "        self.decoder.to(device)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x_flatten = x.view(self.batch_size,-1)\n",
    "        mean = self.encoder_mu(x_flatten)\n",
    "        log_var = self.encoder_sigma(x_flatten)\n",
    "        noise = torch.randn(self.batch_size,mean.shape[1]).to(self.device)\n",
    "        z = mean + torch.exp(0.5*log_var).mul(noise)\n",
    "        return mean, log_var, z\n",
    "        \n",
    "    def decode(self,z):\n",
    "        reconstructed_data = self.decoder(z)\n",
    "        return reconstructed_data\n",
    "\n",
    "    def loss(self,mean,log_var,sample_data,reconstructed_data):\n",
    "        # Reconstruction loss \n",
    "        recon_loss = F.mse_loss(sample_data, reconstructed_data, reduction='sum')\n",
    "        # KL divergence\n",
    "        kl_loss = 0.5 * torch.sum(mean.pow(2) + log_var.exp() - 1 - log_var)\n",
    "        # Total VAE loss\n",
    "        loss = recon_loss + kl_loss\n",
    "        return loss\n",
    "    \n",
    "def train(model,optimizer):\n",
    "    for i in range(model.epoch):\n",
    "        # Sample time indices of size equal to the batch size.\n",
    "        # From sefl.x_aug_sig\n",
    "        time_indics = sample_indices(model.x_aug_sig.shape[0],model.batch_size,\"cuda\")\n",
    "        sample_data = model.x_aug_sig[time_indics]\n",
    "        # Encode \n",
    "        mean, log_var, z = model.encode(sample_data)\n",
    "        # Decode\n",
    "        reconstructed_data = model.decode(z)\n",
    "        # Calculate loss\n",
    "        loss = model.loss(mean,log_var,sample_data.view(model.batch_size,-1),reconstructed_data)\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss\n",
    "        if i%10==0:\n",
    "            print(\"Epcho {} loss {}\".format(i,loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inpust tensor shape: torch.Size([985, 39, 4])\n",
      "VAE(\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=156, out_features=60, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=60, out_features=20, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (encoder_sigma): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=156, out_features=60, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=60, out_features=20, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=60, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=60, out_features=156, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 128\n",
    "epoch = 100\n",
    "hidden_dims = [input_dim,60,20]\n",
    "VAE = VAE(x_aug_sig=x_aug_sig,epoch=epoch,batch_size=batch_size,hidden_dims=hidden_dims,device='cuda')\n",
    "print(VAE)\n",
    "optimizer = torch.optim.Adam(VAE.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcho 0 loss nan\n",
      "Epcho 1 loss nan\n",
      "Epcho 2 loss nan\n",
      "Epcho 3 loss nan\n",
      "Epcho 4 loss nan\n",
      "Epcho 5 loss nan\n",
      "Epcho 6 loss nan\n",
      "Epcho 7 loss nan\n",
      "Epcho 8 loss nan\n",
      "Epcho 9 loss nan\n",
      "Epcho 10 loss nan\n",
      "Epcho 11 loss nan\n",
      "Epcho 12 loss nan\n",
      "Epcho 13 loss nan\n",
      "Epcho 14 loss nan\n",
      "Epcho 15 loss nan\n",
      "Epcho 16 loss nan\n",
      "Epcho 17 loss nan\n",
      "Epcho 18 loss nan\n",
      "Epcho 19 loss nan\n",
      "Epcho 20 loss nan\n",
      "Epcho 21 loss nan\n",
      "Epcho 22 loss nan\n",
      "Epcho 23 loss nan\n",
      "Epcho 24 loss nan\n",
      "Epcho 25 loss nan\n",
      "Epcho 26 loss nan\n",
      "Epcho 27 loss nan\n",
      "Epcho 28 loss nan\n",
      "Epcho 29 loss nan\n",
      "Epcho 30 loss nan\n",
      "Epcho 31 loss nan\n",
      "Epcho 32 loss nan\n",
      "Epcho 33 loss nan\n",
      "Epcho 34 loss nan\n",
      "Epcho 35 loss nan\n",
      "Epcho 36 loss nan\n",
      "Epcho 37 loss nan\n",
      "Epcho 38 loss nan\n",
      "Epcho 39 loss nan\n",
      "Epcho 40 loss nan\n",
      "Epcho 41 loss nan\n",
      "Epcho 42 loss nan\n",
      "Epcho 43 loss nan\n",
      "Epcho 44 loss nan\n",
      "Epcho 45 loss nan\n",
      "Epcho 46 loss nan\n",
      "Epcho 47 loss nan\n",
      "Epcho 48 loss nan\n",
      "Epcho 49 loss nan\n",
      "Epcho 50 loss nan\n",
      "Epcho 51 loss nan\n",
      "Epcho 52 loss nan\n",
      "Epcho 53 loss nan\n",
      "Epcho 54 loss nan\n",
      "Epcho 55 loss nan\n",
      "Epcho 56 loss nan\n",
      "Epcho 57 loss nan\n",
      "Epcho 58 loss nan\n",
      "Epcho 59 loss nan\n",
      "Epcho 60 loss nan\n",
      "Epcho 61 loss nan\n",
      "Epcho 62 loss nan\n",
      "Epcho 63 loss nan\n",
      "Epcho 64 loss nan\n",
      "Epcho 65 loss nan\n",
      "Epcho 66 loss nan\n",
      "Epcho 67 loss nan\n",
      "Epcho 68 loss nan\n",
      "Epcho 69 loss nan\n",
      "Epcho 70 loss nan\n",
      "Epcho 71 loss nan\n",
      "Epcho 72 loss nan\n",
      "Epcho 73 loss nan\n",
      "Epcho 74 loss nan\n",
      "Epcho 75 loss nan\n",
      "Epcho 76 loss nan\n",
      "Epcho 77 loss nan\n",
      "Epcho 78 loss nan\n",
      "Epcho 79 loss nan\n",
      "Epcho 80 loss nan\n",
      "Epcho 81 loss nan\n",
      "Epcho 82 loss nan\n",
      "Epcho 83 loss nan\n",
      "Epcho 84 loss nan\n",
      "Epcho 85 loss nan\n",
      "Epcho 86 loss nan\n",
      "Epcho 87 loss nan\n",
      "Epcho 88 loss nan\n",
      "Epcho 89 loss nan\n",
      "Epcho 90 loss nan\n",
      "Epcho 91 loss nan\n",
      "Epcho 92 loss nan\n",
      "Epcho 93 loss nan\n",
      "Epcho 94 loss nan\n",
      "Epcho 95 loss nan\n",
      "Epcho 96 loss nan\n",
      "Epcho 97 loss nan\n",
      "Epcho 98 loss nan\n",
      "Epcho 99 loss nan\n"
     ]
    }
   ],
   "source": [
    "train(VAE,optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
