{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from lib.utils import sample_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    # Input dimension of encoder\n",
    "    # We'll flat the tensor\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.betavae import BetaVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer):\n",
    "    early_stop = 500\n",
    "    cnt = 0\n",
    "    min_loss = float('inf')\n",
    "    for i in range(model.epoch):\n",
    "        # Sample time indices of size equal to the batch size\n",
    "        # From sefl.x_aug_sig\n",
    "        time_indics = sample_indices(model.x_aug_sig.shape[0],model.batch_size,\"cuda\")\n",
    "        sample_data = model.x_aug_sig[time_indics]\n",
    "        # print(\"sample_data shape {}\".format(sample_data.shape))\n",
    "        # Encode \n",
    "        mean, log_var, z = model.encode(sample_data)\n",
    "        # Decode\n",
    "        reconstructed_data = model.decode(z)\n",
    "        # print(\"reconstructed_data shape {},\".format(reconstructed_data.shape))\n",
    "        # Calculate loss\n",
    "        loss = model.loss(mean,log_var,sample_data.view(sample_data.shape[0],-1),reconstructed_data)\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch {} loss {}\".format(i,loss.item()))\n",
    "        # Early stop\n",
    "        if loss.item()<min_loss:\n",
    "            min_loss = loss.item()\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt>early_stop:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([985, 39, 4])\n",
      "BetaVAE(\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (encoder_sigma): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=156, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 200\n",
    "epoch = 20001\n",
    "hidden_dims = [input_dim,12,3]\n",
    "model = BetaVAE(x_aug_sig=x_aug_sig,epoch=epoch,batch_size=batch_size,hidden_dims=hidden_dims,device='cuda')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1388615168.0\n",
      "Epoch 500 loss 620130048.0\n",
      "Epoch 1000 loss 239009792.0\n",
      "Epoch 1500 loss 117220048.0\n",
      "Epoch 2000 loss 82478944.0\n",
      "Epoch 2500 loss 65292888.0\n",
      "Epoch 3000 loss 51641512.0\n",
      "Epoch 3500 loss 31787168.0\n",
      "Epoch 4000 loss 21586546.0\n",
      "Epoch 4500 loss 16772200.0\n",
      "Epoch 5000 loss 13256212.0\n",
      "Epoch 5500 loss 11331558.0\n",
      "Epoch 6000 loss 8656450.0\n",
      "Epoch 6500 loss 7749748.0\n",
      "Epoch 7000 loss 6225666.0\n",
      "Epoch 7500 loss 5783278.5\n",
      "Epoch 8000 loss 5003040.5\n",
      "Epoch 8500 loss 4337531.5\n",
      "Epoch 9000 loss 3917640.0\n",
      "Epoch 9500 loss 3312057.25\n",
      "Epoch 10000 loss 3073944.0\n",
      "Epoch 10500 loss 2675198.75\n",
      "Epoch 11000 loss 2348293.5\n",
      "Epoch 11500 loss 2090581.75\n",
      "Epoch 12000 loss 1896888.75\n",
      "Epoch 12500 loss 1787290.5\n",
      "Epoch 13000 loss 1591536.0\n",
      "Epoch 13500 loss 1561827.5\n",
      "Epoch 14000 loss 1546347.375\n",
      "Epoch 14500 loss 1443865.0\n",
      "Epoch 15000 loss 1275252.0\n",
      "Epoch 15500 loss 1195807.125\n",
      "Epoch 16000 loss 1124711.75\n",
      "Epoch 16500 loss 1118846.125\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
