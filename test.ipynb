{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from lib.utils import sample_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    # Input dimension of encoder\n",
    "    # We'll flat the tensor\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper___cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(cvae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mCAVE_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/VAE/models/cvae.py:84\u001b[0m, in \u001b[0;36mCAVE_train\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     81\u001b[0m condition_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mx_original[time_indices]  \u001b[38;5;66;03m# [batch_size, 39, 4]\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m mean, log_var, z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m reconstructed_data \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(z, condition_data)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/VAE/models/cvae.py:39\u001b[0m, in \u001b[0;36mCVAE.encode\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     36\u001b[0m x_flatten \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten input [batch_size, 156]\u001b[39;00m\n\u001b[1;32m     37\u001b[0m c_flatten \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mview(c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten condition [batch_size, 156]\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x_concat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_flatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_flatten\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Concatenate input and condition [batch_size, 312]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_fc(x_concat))\n\u001b[1;32m     42\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_mean(hidden)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper___cat)"
     ]
    }
   ],
   "source": [
    "from models.cvae import CVAE,CAVE_train\n",
    "\n",
    "# Model hyperparameters\n",
    "lr = 1e-3  \n",
    "batch_size = 128\n",
    "epoch = 20001\n",
    "latent_dim = 20  \n",
    "hidden_dims = [156, 100, latent_dim]  \n",
    "\n",
    "cvae = CVAE(\n",
    "    x_aug_sig=x_aug_sig,\n",
    "    x_original=x_real_train.to(\"cuda\"),  \n",
    "    epoch=epoch,\n",
    "    batch_size=batch_size,\n",
    "    hidden_dims=hidden_dims,\n",
    "    latent_dim=latent_dim,\n",
    "    device='cuda'\n",
    ")\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "CAVE_train(cvae, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
