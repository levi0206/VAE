{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from lib.datasets import get_stock_price,sample_indices,train_test_split\n",
    "from lib.aug import apply_augmentations,parse_augmentations\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from lib.utils import sample_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    \"ticker\" : \"^GSPC\",\n",
    "    \"interval\" : \"1d\",\n",
    "    \"column\" : 1,  \n",
    "    \"window_size\" : 20,\n",
    "    \"dir\" : \"datasets\",\n",
    "    \"subdir\" : \"stock\"\n",
    "}\n",
    "sig_config = {\n",
    "    \"augmentations\": [\n",
    "        {\"name\": \"AddTime\"},\n",
    "        {\"name\": \"LeadLag\"},\n",
    "    ],\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"depth\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolled data for training, shape torch.Size([1232, 20, 1])\n",
      "Before augmentation shape: torch.Size([985, 20, 1])\n",
      "torch.Size([985, 20, 2])\n",
      "torch.Size([985, 39, 4])\n",
      "After augmentation shape: torch.Size([985, 39, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_data = get_stock_price(data_config)\n",
    "x_real_train, x_real_test = train_test_split(tensor_data, train_test_ratio=0.8, device=sig_config[\"device\"])\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    sig_config[\"augmentations\"] = parse_augmentations(sig_config.get('augmentations'))\n",
    "print(\"Before augmentation shape:\",x_real_train.shape)\n",
    "if sig_config[\"augmentations\"] is not None:\n",
    "    # Print the tensor shape after each augmentation\n",
    "    x_aug_sig = apply_augmentations(x_real_train,sig_config[\"augmentations\"])\n",
    "    # Input dimension of encoder\n",
    "    # We'll flat the tensor\n",
    "    input_dim = x_aug_sig.shape[1]*x_aug_sig.shape[2]\n",
    "print(\"After augmentation shape:\",x_aug_sig.shape)\n",
    "x_aug_sig = x_aug_sig.to(sig_config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, x_aug_sig, epoch, batch_size, hidden_dims: List, device) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.x_aug_sig = x_aug_sig\n",
    "        print(\"Input tensor shape: {}\".format(x_aug_sig.shape))\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Assume len(hidden_dims)=3.\n",
    "        self.encoder_mu = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.encoder_sigma = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[0],hidden_dims[1]),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[2]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[2],hidden_dims[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dims[1],hidden_dims[0]),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        # To device\n",
    "        self.encoder_mu.to(device)\n",
    "        self.encoder_sigma.to(device)\n",
    "        self.decoder.to(device)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x_flatten = x.view(x.shape[0],-1)\n",
    "        mean = self.encoder_mu(x_flatten)\n",
    "        log_var = self.encoder_sigma(x_flatten)\n",
    "        # Clipping\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        noise = torch.randn(x.shape[0],mean.shape[1]).to(self.device)\n",
    "        z = mean + torch.exp(0.5*log_var).mul(noise)\n",
    "        return mean, log_var, z\n",
    "        \n",
    "    def decode(self,z):\n",
    "        reconstructed_data = self.decoder(z)\n",
    "        return reconstructed_data\n",
    "\n",
    "    def loss(self, mean, log_var, sample_data, reconstructed_data, beta=1.0):\n",
    "        \"\"\"\n",
    "        Compute β-VAE loss\n",
    "        \n",
    "        Args:\n",
    "            mean: latent mean [batch_size, latent_dim]\n",
    "            log_var: latent log variance [batch_size, latent_dim]\n",
    "            sample_data: input data [batch_size, 156]\n",
    "            reconstructed_data: reconstructed data [batch_size, 156]\n",
    "            beta: weight for KL divergence term (β parameter)\n",
    "        \n",
    "        Returns:\n",
    "            total_loss: total β-VAE loss\n",
    "            recon_loss: reconstruction loss\n",
    "            kl_loss: KL divergence term\n",
    "        \"\"\"\n",
    "        # Flatten sample_data if not already flattened\n",
    "        sample_data = sample_data.view(sample_data.size(0), -1)  # [128, 156]\n",
    "        batch_size = sample_data.size(0)\n",
    "\n",
    "        # 1. Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(reconstructed_data, sample_data, reduction='sum') / batch_size\n",
    "        \n",
    "        # 2. KL divergence between q(z|x) and p(z)\n",
    "        # Analytical KL divergence: D_KL(N(μ, σ^2) || N(0, 1))\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()) / batch_size\n",
    "        \n",
    "        # Total loss = reconstruction + β * KL\n",
    "        total_loss = recon_loss + beta * kl_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def generate(self,x: torch.Tensor):\n",
    "        _, _, z = self.encode(x)\n",
    "        reconstructed_data = self.decode(z)\n",
    "        return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer):\n",
    "    early_stop = 500\n",
    "    cnt = 0\n",
    "    min_loss = float('inf')\n",
    "    for i in range(model.epoch):\n",
    "        # Sample time indices of size equal to the batch size\n",
    "        # From sefl.x_aug_sig\n",
    "        time_indics = sample_indices(model.x_aug_sig.shape[0],model.batch_size,\"cuda\")\n",
    "        sample_data = model.x_aug_sig[time_indics]\n",
    "        # print(\"sample_data shape {}\".format(sample_data.shape))\n",
    "        # Encode \n",
    "        mean, log_var, z = model.encode(sample_data)\n",
    "        # Decode\n",
    "        reconstructed_data = model.decode(z)\n",
    "        # print(\"reconstructed_data shape {},\".format(reconstructed_data.shape))\n",
    "        # Calculate loss\n",
    "        loss = model.loss(mean,log_var,sample_data.view(sample_data.shape[0],-1),reconstructed_data)\n",
    "        # Backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print loss\n",
    "        if i%500==0:\n",
    "            print(\"Epoch {} loss {}\".format(i,loss.item()))\n",
    "        # Early stop\n",
    "        if loss.item()<min_loss:\n",
    "            min_loss = loss.item()\n",
    "            cnt = 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt>early_stop:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([985, 39, 4])\n",
      "BetaVAE(\n",
      "  (encoder_mu): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (encoder_sigma): Sequential(\n",
      "    (0): Linear(in_features=156, out_features=12, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=12, out_features=3, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=12, out_features=156, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 200\n",
    "epoch = 20001\n",
    "hidden_dims = [input_dim,12,3]\n",
    "model = BetaVAE(x_aug_sig=x_aug_sig,epoch=epoch,batch_size=batch_size,hidden_dims=hidden_dims,device='cuda')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1463627392.0\n",
      "Epoch 500 loss 434325632.0\n",
      "Epoch 1000 loss 127512672.0\n",
      "Epoch 1500 loss 65280000.0\n",
      "Epoch 2000 loss 44557572.0\n",
      "Epoch 2500 loss 32601516.0\n",
      "Epoch 3000 loss 25540910.0\n",
      "Epoch 3500 loss 20314860.0\n",
      "Epoch 4000 loss 16063943.0\n",
      "Epoch 4500 loss 13197256.0\n",
      "Epoch 5000 loss 10956188.0\n",
      "Epoch 5500 loss 8774265.0\n",
      "Epoch 6000 loss 7202094.5\n",
      "Epoch 6500 loss 5849141.0\n",
      "Epoch 7000 loss 4701825.0\n",
      "Epoch 7500 loss 3961849.5\n",
      "Epoch 8000 loss 3127251.25\n",
      "Epoch 8500 loss 2629494.5\n",
      "Epoch 9000 loss 2347202.0\n",
      "Epoch 9500 loss 2043580.5\n",
      "Epoch 10000 loss 1808552.125\n",
      "Epoch 10500 loss 1769922.625\n",
      "Epoch 11000 loss 1624679.75\n",
      "Epoch 11500 loss 1464341.875\n",
      "Epoch 12000 loss 1426667.0\n",
      "Epoch 12500 loss 1340772.25\n",
      "Epoch 13000 loss 1284507.0\n",
      "Epoch 13500 loss 1224837.375\n",
      "Epoch 14000 loss 1146381.0\n",
      "Epoch 14500 loss 1310418.875\n",
      "Epoch 15000 loss 1276603.5\n",
      "Epoch 15500 loss 1247635.875\n"
     ]
    }
   ],
   "source": [
    "train(model,optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siggan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
